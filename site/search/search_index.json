{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Enterprise-grade integration with Cloud Pak for Integration v2022.4 \u00b6 Welcome to our workshop! In this workshop we'll be looking at various parts of the Cloud Pak for Integration platform. The goals of this workshop are: Create, deploy and test a new API using the IBM API Connect Developer Toolkit Using IBM Event Streams for near realtime data replication About this workshop \u00b6 The introductory page of the workshop is broken down into the following sections: About Cloud Pak for Integration Credits About Cloud Pak for Integration \u00b6 IBM Cloud Pak for Integration is an enterprise-ready, containerized software solution that contains all the tools you need to integrate and connect application components and data both within and between clouds. Running on Red Hat OpenShift, the IBM Cloud Pak for Integration gives businesses complete choice and agility to deploy workloads on premises and on private and public cloud. Cloud Pak for Integration includes components to enable you to manage: API lifecycle Create, secure, manage, share and monetize APIs across clouds while you maintain continuous availability. Take control of your API ecosystem and drive digital business with a robust API strategy that can meet the changing needs of your users Application and data integration Integrate all of your business data and applications more quickly and easily across any cloud, from the simplest SaaS application to the most complex systems - without worrying about mismatched sources, formats or standards Enterprise messaging Simplify, accelerate and facilitate the reliable exchange of data with a flexible and security-rich messaging solution that\u2019s trusted by some of the world\u2019s most successful enterprises. Ensure you receive the information you need, when you need it - and receive it only once Event streaming Use IBM Event Streams to deliver messages more easily and reliably and to react to events in real time. Provide more personalized customer experiences by responding to events before the moment passes High-speed data transfer Send large files and data sets virtually anywhere, reliably and at maximum speed. Accelerate collaboration and meet the demands of complex global teams, without compromising performance or security Credits \u00b6 This workshop was primarily written by David Carew","title":"About the workshop"},{"location":"#enterprise-grade-integration-with-cloud-pak-for-integration-v20224","text":"Welcome to our workshop! In this workshop we'll be looking at various parts of the Cloud Pak for Integration platform. The goals of this workshop are: Create, deploy and test a new API using the IBM API Connect Developer Toolkit Using IBM Event Streams for near realtime data replication","title":"Enterprise-grade integration with Cloud Pak for Integration v2022.4"},{"location":"#about-this-workshop","text":"The introductory page of the workshop is broken down into the following sections: About Cloud Pak for Integration Credits","title":"About this workshop"},{"location":"#about-cloud-pak-for-integration","text":"IBM Cloud Pak for Integration is an enterprise-ready, containerized software solution that contains all the tools you need to integrate and connect application components and data both within and between clouds. Running on Red Hat OpenShift, the IBM Cloud Pak for Integration gives businesses complete choice and agility to deploy workloads on premises and on private and public cloud. Cloud Pak for Integration includes components to enable you to manage: API lifecycle Create, secure, manage, share and monetize APIs across clouds while you maintain continuous availability. Take control of your API ecosystem and drive digital business with a robust API strategy that can meet the changing needs of your users Application and data integration Integrate all of your business data and applications more quickly and easily across any cloud, from the simplest SaaS application to the most complex systems - without worrying about mismatched sources, formats or standards Enterprise messaging Simplify, accelerate and facilitate the reliable exchange of data with a flexible and security-rich messaging solution that\u2019s trusted by some of the world\u2019s most successful enterprises. Ensure you receive the information you need, when you need it - and receive it only once Event streaming Use IBM Event Streams to deliver messages more easily and reliably and to react to events in real time. Provide more personalized customer experiences by responding to events before the moment passes High-speed data transfer Send large files and data sets virtually anywhere, reliably and at maximum speed. Accelerate collaboration and meet the demands of complex global teams, without compromising performance or security","title":"About Cloud Pak for Integration"},{"location":"#credits","text":"This workshop was primarily written by David Carew","title":"Credits"},{"location":"exercise-api-connect/","text":"Exercise - Create, deploy and test a new API using the API Connect Developer Toolkit \u00b6 In this lab you will create a new API using the OpenAPI definition of an existing RESTful web-service that gets realtime stock quotes. You will then test the deployed API by deploying the IBM Trader Lite application which is a simple stock trading sample, written as a set of microservices. The app uses the API definition that you will create to get realtime stock quotes. The architecture of the app is shown below: Tradr is a Node.js UI for the portfolio service The portfolio microservice sits at the center of the application. This microservice: persists trade data using JDBC to a MariaDB database invokes the stock-quote service that invokes an API defined in API Connect in CP4I to get stock quotes calls the trade-history service to store trade data in a PostgreSQL database that can be queried for reporting purposes. calls the trade-history service to get aggregated historical trade data. This lab is broken up into the following steps: Download the OpenAPI definition file for the external Stock Quote service Import the OpenAPI definition file into API Manager Configure the API Test the API Install the TraderLite app Verify that the Trader Lite app is calling your API successfully Summary Step 1: Download the OpenAPI definition file for the external Stock Quote service \u00b6 Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous state. 1.1 In your browser right click on the following link, right click and select Save Link As ... from the context menu. Save the file stock-quote-api.yaml to your local system. stock-quote-api.yaml Step 2: Import the OpenAPI definition file into API Manager \u00b6 2.1 Go to the CP4I Platform Navigator browser tab Note : If you've closed the Platform Navigator tab in your browser follow the instructions in the FAQ . 2.2 Click on the link to the API Connect instance 2.3 Select the OpenLDAP Local user registry 2.4 Log in with the same credentials your were given for the workshop. Note: The API Connect application can take a long time to load when you use it for the first time. 2.5 Click on the Develop APIs and Products tile 2.6 Click Add and select API (from REST,GraphQL or SOAP) from the context menu 2.7 On the next screen select Existing OpenAPI under Import and then click Next . 2.8 Now choose stock-quote-api.yaml from your local file system and click Next . 2.9 Do not select Activate API . Click Next 2.10 The API should be imported successfully as shown below. Click Edit API . Step 3: Configure the API \u00b6 After importing the existing API, the first step is to configure basic security before exposing it to other developers. By creating a client key you are able to identify the app using the services. Next, we will define the backend endpoints where the API is actually running. API Connect supports pointing to multiple backend endpoints to match your multiple build stage environments. 3.1 In the left navigation select Host and replace the hard coded endpoint address with $(catalog.host) to indicate that you want calls to the external API to go through API Connect. 3.2 Click Save 3.3 In the Edit API screen click Security Schemes(0) in the left navigation 3.4 In the Security section, click the Add button on the right and then click on Create a security scheme . 3.5 In the Security Scheme Name(Key) field, type client-id . 3.6 Under Security Definition Type , choose apiKey . 3.6 Under Key Type , choose client_id . 3.7 For Located In choose header . 3.8 Enter X-IBM-Client-Id as the Variable Name . Your screen should now look like the image below. 3.9 Click the Create button and then click Save . 3.10 Next you'll require use of the Client Id to access your API. In the left Navigation select Security(0) and then click on Create a Security Requirement 3.11 Select the Security Scheme you just created and the click Create . 3.12 Click Save 3.13 Next you'll define the endpoint for the external API. Select the Gateway tab, expand Properties in the left navigation. 3.14 Click on the target-url property. 3.15 Copy then paste the following URL into the Property Value field: https://stocktrader.ibmc.buildlab.cloud 3.16 Click Save to complete the configuration. Step 4: Test the API \u00b6 In the API designer, you have the ability to test the API immediately after creation. 4.1 On the left Navigation, click Policies . 4.2 Click invoke in the flow designer. Note the window on the right with the configuration. The invoke node calls the target-url (ie the external service). 4.3 Modify the URL field to include the request path passed in by the caller as well by appending $(request.path) to the URL . When you're done the field should be set to: $(target-url)$(request.path) 4.4 Click Save 4.5 Toggle the Offline switch and then click on the Test tab 4.6 The Request should be prefilled with the GET request to /stock-quote/djia . 4.7 Note that your client-id is prefilled for you. 4.8 Click Send . 4.9 If this is the first test of the API, you may see a certificate exception. Simply click on the link provided. This will open a new tab and allow you to click through to accept the self signed certificate. Note : Stop when you get a 401 error code in the new tab. 4.10 Go back to the previous tab and click Send again. 4.11 Now you should see a Response section with Status code 200 OK and the Body displaying the details of the simulated Dow Jones Industrial Average . 4.12 Next you'll get the Client Id and Gateway endpoint so you can test your API from the TraderLite app. Click on the Endpoint tab. 4.13 Copy the value of the api-gateway-service URL and the Client-Id to a local text file so it can be used in the Stock Trader application later ( Note: this is a shortcut to the regular process of publishing the API and then subscribing to it as a consumer). Step 5: Install the TraderLite app \u00b6 5.1 In a separate browser tab go to the OpenShift console URL for the cluster assigned to you by for the workshop. Note : There is a link to your assigned cluster's console on your Workshop Information page. If you have closed it, you can access it following the instructions in the FAQ . 5.2 Click on Projects in the left navigation and then click on your studentnnn project in the list 5.3 Click on Installed Operators (in the Operators section) in the left navigation and then click on the TraderLite Operator in the list. 5.4 Click the Create Instance to start the installation of the TraderLite app. 5.5 Name the instance traderlite 5.6 Scroll down the page to the Stock Quote Microservice and replace the API Connect URL and API Connect ClientId with the api-gateway-service URL and the Client-Id you saved in the previous section. Click Create 5.7 In the left navigation select Pods (in the Workloads section) and then wait for all the TraderLite pods to have a status of Running and be in the Ready state. Note: You will know the traderlite-xxxxx pods are in a ready state when the Ready column shows 1/1 . Step 6: Verify that the Trader Lite app is calling your API successfully \u00b6 6.1 In your OpenShift console click on Routes (in the Networking section) in the left navigation and then click on the icon next to the url for the tradr app (the UI for TraderLite) 6.2 Log in using the username stock and the password trader 6.3 If the simulated DJIA summary has data then congratulations ! It means that the API you created in API Connect is working ! Summary \u00b6 Congratulations ! You successfully completed the following key steps in this lab: Created an API by importing an OpenAPI definition for an existing REST service. Configured a ClientID/API Key for security set up a proxy to the existing API. Tested the API in the API Connect developer toolkit. Deployed the Trader Lite app and configured it to use the API you created. Tested the Trader Lite app to make sure it successfully uses your API.","title":"Lab - Create, deploy and test an API using API Connect"},{"location":"exercise-api-connect/#exercise-create-deploy-and-test-a-new-api-using-the-api-connect-developer-toolkit","text":"In this lab you will create a new API using the OpenAPI definition of an existing RESTful web-service that gets realtime stock quotes. You will then test the deployed API by deploying the IBM Trader Lite application which is a simple stock trading sample, written as a set of microservices. The app uses the API definition that you will create to get realtime stock quotes. The architecture of the app is shown below: Tradr is a Node.js UI for the portfolio service The portfolio microservice sits at the center of the application. This microservice: persists trade data using JDBC to a MariaDB database invokes the stock-quote service that invokes an API defined in API Connect in CP4I to get stock quotes calls the trade-history service to store trade data in a PostgreSQL database that can be queried for reporting purposes. calls the trade-history service to get aggregated historical trade data. This lab is broken up into the following steps: Download the OpenAPI definition file for the external Stock Quote service Import the OpenAPI definition file into API Manager Configure the API Test the API Install the TraderLite app Verify that the Trader Lite app is calling your API successfully Summary","title":"Exercise - Create, deploy and test a new API using the API Connect Developer Toolkit"},{"location":"exercise-api-connect/#step-1-download-the-openapi-definition-file-for-the-external-stock-quote-service","text":"Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous state. 1.1 In your browser right click on the following link, right click and select Save Link As ... from the context menu. Save the file stock-quote-api.yaml to your local system. stock-quote-api.yaml","title":"Step 1: Download the OpenAPI definition file for the external Stock Quote service"},{"location":"exercise-api-connect/#step-2-import-the-openapi-definition-file-into-api-manager","text":"2.1 Go to the CP4I Platform Navigator browser tab Note : If you've closed the Platform Navigator tab in your browser follow the instructions in the FAQ . 2.2 Click on the link to the API Connect instance 2.3 Select the OpenLDAP Local user registry 2.4 Log in with the same credentials your were given for the workshop. Note: The API Connect application can take a long time to load when you use it for the first time. 2.5 Click on the Develop APIs and Products tile 2.6 Click Add and select API (from REST,GraphQL or SOAP) from the context menu 2.7 On the next screen select Existing OpenAPI under Import and then click Next . 2.8 Now choose stock-quote-api.yaml from your local file system and click Next . 2.9 Do not select Activate API . Click Next 2.10 The API should be imported successfully as shown below. Click Edit API .","title":"Step 2: Import the OpenAPI definition file into API Manager"},{"location":"exercise-api-connect/#step-3-configure-the-api","text":"After importing the existing API, the first step is to configure basic security before exposing it to other developers. By creating a client key you are able to identify the app using the services. Next, we will define the backend endpoints where the API is actually running. API Connect supports pointing to multiple backend endpoints to match your multiple build stage environments. 3.1 In the left navigation select Host and replace the hard coded endpoint address with $(catalog.host) to indicate that you want calls to the external API to go through API Connect. 3.2 Click Save 3.3 In the Edit API screen click Security Schemes(0) in the left navigation 3.4 In the Security section, click the Add button on the right and then click on Create a security scheme . 3.5 In the Security Scheme Name(Key) field, type client-id . 3.6 Under Security Definition Type , choose apiKey . 3.6 Under Key Type , choose client_id . 3.7 For Located In choose header . 3.8 Enter X-IBM-Client-Id as the Variable Name . Your screen should now look like the image below. 3.9 Click the Create button and then click Save . 3.10 Next you'll require use of the Client Id to access your API. In the left Navigation select Security(0) and then click on Create a Security Requirement 3.11 Select the Security Scheme you just created and the click Create . 3.12 Click Save 3.13 Next you'll define the endpoint for the external API. Select the Gateway tab, expand Properties in the left navigation. 3.14 Click on the target-url property. 3.15 Copy then paste the following URL into the Property Value field: https://stocktrader.ibmc.buildlab.cloud 3.16 Click Save to complete the configuration.","title":"Step 3: Configure the API"},{"location":"exercise-api-connect/#step-4-test-the-api","text":"In the API designer, you have the ability to test the API immediately after creation. 4.1 On the left Navigation, click Policies . 4.2 Click invoke in the flow designer. Note the window on the right with the configuration. The invoke node calls the target-url (ie the external service). 4.3 Modify the URL field to include the request path passed in by the caller as well by appending $(request.path) to the URL . When you're done the field should be set to: $(target-url)$(request.path) 4.4 Click Save 4.5 Toggle the Offline switch and then click on the Test tab 4.6 The Request should be prefilled with the GET request to /stock-quote/djia . 4.7 Note that your client-id is prefilled for you. 4.8 Click Send . 4.9 If this is the first test of the API, you may see a certificate exception. Simply click on the link provided. This will open a new tab and allow you to click through to accept the self signed certificate. Note : Stop when you get a 401 error code in the new tab. 4.10 Go back to the previous tab and click Send again. 4.11 Now you should see a Response section with Status code 200 OK and the Body displaying the details of the simulated Dow Jones Industrial Average . 4.12 Next you'll get the Client Id and Gateway endpoint so you can test your API from the TraderLite app. Click on the Endpoint tab. 4.13 Copy the value of the api-gateway-service URL and the Client-Id to a local text file so it can be used in the Stock Trader application later ( Note: this is a shortcut to the regular process of publishing the API and then subscribing to it as a consumer).","title":"Step 4: Test the API"},{"location":"exercise-api-connect/#step-5-install-the-traderlite-app","text":"5.1 In a separate browser tab go to the OpenShift console URL for the cluster assigned to you by for the workshop. Note : There is a link to your assigned cluster's console on your Workshop Information page. If you have closed it, you can access it following the instructions in the FAQ . 5.2 Click on Projects in the left navigation and then click on your studentnnn project in the list 5.3 Click on Installed Operators (in the Operators section) in the left navigation and then click on the TraderLite Operator in the list. 5.4 Click the Create Instance to start the installation of the TraderLite app. 5.5 Name the instance traderlite 5.6 Scroll down the page to the Stock Quote Microservice and replace the API Connect URL and API Connect ClientId with the api-gateway-service URL and the Client-Id you saved in the previous section. Click Create 5.7 In the left navigation select Pods (in the Workloads section) and then wait for all the TraderLite pods to have a status of Running and be in the Ready state. Note: You will know the traderlite-xxxxx pods are in a ready state when the Ready column shows 1/1 .","title":"Step 5: Install the TraderLite app"},{"location":"exercise-api-connect/#step-6-verify-that-the-trader-lite-app-is-calling-your-api-successfully","text":"6.1 In your OpenShift console click on Routes (in the Networking section) in the left navigation and then click on the icon next to the url for the tradr app (the UI for TraderLite) 6.2 Log in using the username stock and the password trader 6.3 If the simulated DJIA summary has data then congratulations ! It means that the API you created in API Connect is working !","title":"Step 6: Verify that the Trader Lite app is calling your API successfully"},{"location":"exercise-api-connect/#summary","text":"Congratulations ! You successfully completed the following key steps in this lab: Created an API by importing an OpenAPI definition for an existing REST service. Configured a ClientID/API Key for security set up a proxy to the existing API. Tested the API in the API Connect developer toolkit. Deployed the Trader Lite app and configured it to use the API you created. Tested the Trader Lite app to make sure it successfully uses your API.","title":"Summary"},{"location":"exercise-event-streaming/","text":"Exercise - Using IBM MQ and IBM Event Streams for near realtime data replication \u00b6 In this lab you will use IBM MQ and IBM Event Streams to replicate data from a transactional database to a reporting database. The pattern used allows for seamless horizontal scaling to minimize the latency between the time the transaction is committed to the transactional database and when it is available to be queried in the reporting database. The architecture of the solution you will build is shown below: The portfolio microservice sits at the center of the application. This microservice: sends completed transactions to an IBM MQ queue. calls the trade-history service to get aggregated historical trade data. The Kafka Connect source uses the Kafka Connect framework and an IBM MQ source to consume the transaction data from IBM MQ and sends it to a topic in IBM Event Streams. By scaling this service horizontally you can decrease the latency between the time the transaction is committed to the transactional database and when it is available to be queried in the reporting database, The Kafka Connect sink uses the Kafka Connect framework and a Mongodb sink to receive the transaction data from IBM Event Streams and publishes it to the reporting database. By scaling this service horizontally you can decrease the latency between the time the transaction is committed to the transactional database and when it is available to be queried in the reporting database. This lab is broken up into the following steps: Uninstall the TraderLite app Create a topic in the Event Streams Management Console Add messaging components to the Trader Lite app Generate some test data with the Trader Lite app Verify your trades have been sent as messages to IBM MQ Start Kafka Replication Verify transaction data was replicated to the Trade History database Examine the messages sent to your Event Streams topic Summary Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous state. Step 1: Uninstall the TraderLite app \u00b6 If you've completed the API Connect and/or the Salesforce integration labs then you will have the app running already. For this lab it's easier to install the app from scratch so you can use the OpenShift GUI environment (as opposed to editing the yaml file of an existing instance) to select all the options needed for this app. If the app is NOT installed skip to Step 2 . 1.1 Go to the OpenShift console of your workshop cluster. Select your studentnnn project. In the navigation on the left select Installed Operators in the Operators section and select the TraderLite Operator 1.2 Click on the TraderLite app tab 1.3 Click on the 3 periods to the right of the existing TraderLite CRD and select Delete TraderLite from the context menu. 1.4 In the navigation area on the left select Pods in the Workloads section. You should see that the traderlite-xxxxx-yyyyy pods are terminated. Note: You can enter traderlite in the search by name input field to filter the pods. Step 2: Create a topic in the Event Streams Management Console \u00b6 2.1 Go to the CP4I Platform Navigator browser tab Note : If you've closed the Platform Navigator tab in your browser follow the instructions in the FAQ . 2.2 Click on the link to the Event Streams instance 2.3 If prompted to login select the Enterprise LDAP user registry and log in with your credentials. 2.4 Click on the Create a topic tile 2.5 Use your username for the name of the topic. For example, if your username is student005 then name the topic student005 . Click Next . 2.6 Leave the default for message retention and click Next . 2.7 Leave the default for replicas and click Create topic . 2.8 You should see your new topic listed. Step 3: Add messaging components to the Trader Lite app \u00b6 In this section you will install the TraderLite app to start storing transactions as MQ messages, without setting up the KafkaConnect part that will move the transactions out of MQ, into Event Streams and then into MongoDB. This demonstrates how MQ can serve as a reliable store and forward buffer especially during temporary network disruption. 3.1 Go to the OpenShift console of your workshop cluster. Select your studentnnn project. 3.2 Click on Installed Operators (in the Operators section) in the left navigation and then click on the TraderLite Operator in the list. 3.3 Click the Create Instance to start the installation of the TraderLite app. 3.4 Name the instance traderlite 3.5 Set the following values: Under Kafka Access select the studentnnn topic Enable KafkaIntegration Under Mqaccess select the STUDENTNNN.QUEUE queue that corresponds to your username. 3.6 Scroll down and click Create 3.7 In the left navigation select Pods (in the Workloads section) and then wait for all the TraderLite pods to have a status of Running and be in the Ready state. mkdocs Note: You will know the traderlite-xxxxx pods are in a ready state when the Ready column shows 1/1 . Step 4: Generate some test data with the TraderLite app \u00b6 4.1 In your OpenShift console click on Routes in the left navigation under the Networking section and then click on the icon next to the url for the tradr app (the UI for TraderLite) 4.2 Log in using the username stock and the password trader 4.3 Click Add Client fill in the form. You must use valid email and phone number formats to avoid validation errors. 4.4 Click Save 4.5 Click on the Portfolio ID of the new client to see the details of the portfolio 4.6 Buy some shares of 2 or 3 different companies and then sell a portion of one of the shares you just bought. To buy shares, click the Buy Stock button, then select a company and enter a share amount. To sell shares, click the Sell stock button, then select the company symbol and enter the number of shares to sell. Note: Your ROI will be wrong because we are not yet replicating the historical trade data that goes in to the calculation of the ROI. Step 5: View messages in IBM MQ \u00b6 5.1 Go to the CP4I Platform Navigator browser tab Note : If you've closed the Platform Navigator tab in your browser follow the instructions in the FAQ . 5.2 Click on the link to the MQ instance 5.3 If prompted to login select the Enterprise LDAP user registry and log in with your credentials. 5.4 Click on the Manage QMTRADER tile 5.5 Click on the STUDENTNNN.QUEUE queue that corresponds to your username. 5.5 You should see messages for the trades you just executed. The number of messages in the queue will vary based on the number of buy/sell transactions you performed in the previous steps. 5.6 Keep the browser tab with the MQ web interface open as you'll need it later in the lab. Step 6: Start Kafka Replication \u00b6 In this section you will configure the TraderLite app to start moving the transaction data out of MQ, into Kafka and then into the MongoDB reporting database. 6.1 Go to the OpenShift console of your assigned cluster. In the navigation on the left select Installed Operators and select the TraderLite Operator 6.2 Click on the TraderLite app tab 6.3 Click on the 3 periods to the right of the existing TraderLite CRD and select Edit TraderLite from the context menu. 6.4 Scroll down to line 432 and set Kafka Connect enabled to true (leave all the other values unchanged). 6.5 Click Save . 6.6 In the navigation on the left select Installed Operators and select the Event Streams operator. 6.7 Click on the All instances tab and wait for the mq-source and mongodb-sink connectors to be in the Ready state before continuing. 6.8 Go back to the browser tab with the MQ Console and verify that all the messages have been consumed by the mq-source connector. (Note: You may need to reload this browser tab to see that the messages have been consumed.) Step 7: Verify transaction data was replicated to the Trade History database \u00b6 7.1 Go to the OpenShift console of your workshop cluster. In the navigation on the left select Routes in the Networking section. 7.2 Copy the link for the trade-history microservice and paste it into a new browser tab. Note: You will get a 404 (Not Found) message if you try to access this URL as is. This is because the trade-history microservice requires extra path information. 7.3 Append the string /trades/1000 to the address you pasted - you should get back some JSON with the test transactions that you ran earlier. Step 8: Examine the messages sent to your Event Streams topic \u00b6 8.1 Go to the CP4I Platform Navigator browser tab Note : If you've closed the Platform Navigator tab in your browser follow the instructions in the FAQ . 8.2 Click on the link to the Event Streams instance 8.3 If prompted to login select the Enterprise LDAP user registry and log in with your credentials. 8.4 Click on the topics icon 8.5 Click on th topic that corresponds to your username. 8.6 Select a message to see it's details Summary \u00b6 Congratulations ! You successfully completed the following key steps in this lab: Configured the Trader Lite app to use MQ Deploy Kafka Connect with IBM Event Streams Generated transactions in the Trader Lite app and verified that the data is being replicated via MQ and Event Streams","title":"Lab - Using IBM MQ and Kafka for near realtime data replication"},{"location":"exercise-event-streaming/#exercise-using-ibm-mq-and-ibm-event-streams-for-near-realtime-data-replication","text":"In this lab you will use IBM MQ and IBM Event Streams to replicate data from a transactional database to a reporting database. The pattern used allows for seamless horizontal scaling to minimize the latency between the time the transaction is committed to the transactional database and when it is available to be queried in the reporting database. The architecture of the solution you will build is shown below: The portfolio microservice sits at the center of the application. This microservice: sends completed transactions to an IBM MQ queue. calls the trade-history service to get aggregated historical trade data. The Kafka Connect source uses the Kafka Connect framework and an IBM MQ source to consume the transaction data from IBM MQ and sends it to a topic in IBM Event Streams. By scaling this service horizontally you can decrease the latency between the time the transaction is committed to the transactional database and when it is available to be queried in the reporting database, The Kafka Connect sink uses the Kafka Connect framework and a Mongodb sink to receive the transaction data from IBM Event Streams and publishes it to the reporting database. By scaling this service horizontally you can decrease the latency between the time the transaction is committed to the transactional database and when it is available to be queried in the reporting database. This lab is broken up into the following steps: Uninstall the TraderLite app Create a topic in the Event Streams Management Console Add messaging components to the Trader Lite app Generate some test data with the Trader Lite app Verify your trades have been sent as messages to IBM MQ Start Kafka Replication Verify transaction data was replicated to the Trade History database Examine the messages sent to your Event Streams topic Summary Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous state.","title":"Exercise - Using IBM MQ and IBM Event Streams for near realtime data replication"},{"location":"exercise-event-streaming/#step-1-uninstall-the-traderlite-app","text":"If you've completed the API Connect and/or the Salesforce integration labs then you will have the app running already. For this lab it's easier to install the app from scratch so you can use the OpenShift GUI environment (as opposed to editing the yaml file of an existing instance) to select all the options needed for this app. If the app is NOT installed skip to Step 2 . 1.1 Go to the OpenShift console of your workshop cluster. Select your studentnnn project. In the navigation on the left select Installed Operators in the Operators section and select the TraderLite Operator 1.2 Click on the TraderLite app tab 1.3 Click on the 3 periods to the right of the existing TraderLite CRD and select Delete TraderLite from the context menu. 1.4 In the navigation area on the left select Pods in the Workloads section. You should see that the traderlite-xxxxx-yyyyy pods are terminated. Note: You can enter traderlite in the search by name input field to filter the pods.","title":"Step 1: Uninstall the TraderLite  app"},{"location":"exercise-event-streaming/#step-2-create-a-topic-in-the-event-streams-management-console","text":"2.1 Go to the CP4I Platform Navigator browser tab Note : If you've closed the Platform Navigator tab in your browser follow the instructions in the FAQ . 2.2 Click on the link to the Event Streams instance 2.3 If prompted to login select the Enterprise LDAP user registry and log in with your credentials. 2.4 Click on the Create a topic tile 2.5 Use your username for the name of the topic. For example, if your username is student005 then name the topic student005 . Click Next . 2.6 Leave the default for message retention and click Next . 2.7 Leave the default for replicas and click Create topic . 2.8 You should see your new topic listed.","title":"Step 2: Create a topic in the Event Streams Management Console"},{"location":"exercise-event-streaming/#step-3-add-messaging-components-to-the-trader-lite-app","text":"In this section you will install the TraderLite app to start storing transactions as MQ messages, without setting up the KafkaConnect part that will move the transactions out of MQ, into Event Streams and then into MongoDB. This demonstrates how MQ can serve as a reliable store and forward buffer especially during temporary network disruption. 3.1 Go to the OpenShift console of your workshop cluster. Select your studentnnn project. 3.2 Click on Installed Operators (in the Operators section) in the left navigation and then click on the TraderLite Operator in the list. 3.3 Click the Create Instance to start the installation of the TraderLite app. 3.4 Name the instance traderlite 3.5 Set the following values: Under Kafka Access select the studentnnn topic Enable KafkaIntegration Under Mqaccess select the STUDENTNNN.QUEUE queue that corresponds to your username. 3.6 Scroll down and click Create 3.7 In the left navigation select Pods (in the Workloads section) and then wait for all the TraderLite pods to have a status of Running and be in the Ready state. mkdocs Note: You will know the traderlite-xxxxx pods are in a ready state when the Ready column shows 1/1 .","title":"Step 3: Add messaging components to the Trader Lite app"},{"location":"exercise-event-streaming/#step-4-generate-some-test-data-with-the-traderlite-app","text":"4.1 In your OpenShift console click on Routes in the left navigation under the Networking section and then click on the icon next to the url for the tradr app (the UI for TraderLite) 4.2 Log in using the username stock and the password trader 4.3 Click Add Client fill in the form. You must use valid email and phone number formats to avoid validation errors. 4.4 Click Save 4.5 Click on the Portfolio ID of the new client to see the details of the portfolio 4.6 Buy some shares of 2 or 3 different companies and then sell a portion of one of the shares you just bought. To buy shares, click the Buy Stock button, then select a company and enter a share amount. To sell shares, click the Sell stock button, then select the company symbol and enter the number of shares to sell. Note: Your ROI will be wrong because we are not yet replicating the historical trade data that goes in to the calculation of the ROI.","title":"Step 4: Generate some test data with the TraderLite app"},{"location":"exercise-event-streaming/#step-5-view-messages-in-ibm-mq","text":"5.1 Go to the CP4I Platform Navigator browser tab Note : If you've closed the Platform Navigator tab in your browser follow the instructions in the FAQ . 5.2 Click on the link to the MQ instance 5.3 If prompted to login select the Enterprise LDAP user registry and log in with your credentials. 5.4 Click on the Manage QMTRADER tile 5.5 Click on the STUDENTNNN.QUEUE queue that corresponds to your username. 5.5 You should see messages for the trades you just executed. The number of messages in the queue will vary based on the number of buy/sell transactions you performed in the previous steps. 5.6 Keep the browser tab with the MQ web interface open as you'll need it later in the lab.","title":"Step 5: View messages in IBM MQ"},{"location":"exercise-event-streaming/#step-6-start-kafka-replication","text":"In this section you will configure the TraderLite app to start moving the transaction data out of MQ, into Kafka and then into the MongoDB reporting database. 6.1 Go to the OpenShift console of your assigned cluster. In the navigation on the left select Installed Operators and select the TraderLite Operator 6.2 Click on the TraderLite app tab 6.3 Click on the 3 periods to the right of the existing TraderLite CRD and select Edit TraderLite from the context menu. 6.4 Scroll down to line 432 and set Kafka Connect enabled to true (leave all the other values unchanged). 6.5 Click Save . 6.6 In the navigation on the left select Installed Operators and select the Event Streams operator. 6.7 Click on the All instances tab and wait for the mq-source and mongodb-sink connectors to be in the Ready state before continuing. 6.8 Go back to the browser tab with the MQ Console and verify that all the messages have been consumed by the mq-source connector. (Note: You may need to reload this browser tab to see that the messages have been consumed.)","title":"Step 6: Start Kafka Replication"},{"location":"exercise-event-streaming/#step-7-verify-transaction-data-was-replicated-to-the-trade-history-database","text":"7.1 Go to the OpenShift console of your workshop cluster. In the navigation on the left select Routes in the Networking section. 7.2 Copy the link for the trade-history microservice and paste it into a new browser tab. Note: You will get a 404 (Not Found) message if you try to access this URL as is. This is because the trade-history microservice requires extra path information. 7.3 Append the string /trades/1000 to the address you pasted - you should get back some JSON with the test transactions that you ran earlier.","title":"Step 7: Verify transaction data was replicated to the Trade History database"},{"location":"exercise-event-streaming/#step-8-examine-the-messages-sent-to-your-event-streams-topic","text":"8.1 Go to the CP4I Platform Navigator browser tab Note : If you've closed the Platform Navigator tab in your browser follow the instructions in the FAQ . 8.2 Click on the link to the Event Streams instance 8.3 If prompted to login select the Enterprise LDAP user registry and log in with your credentials. 8.4 Click on the topics icon 8.5 Click on th topic that corresponds to your username. 8.6 Select a message to see it's details","title":"Step 8: Examine the messages sent to your Event Streams topic"},{"location":"exercise-event-streaming/#summary","text":"Congratulations ! You successfully completed the following key steps in this lab: Configured the Trader Lite app to use MQ Deploy Kafka Connect with IBM Event Streams Generated transactions in the Trader Lite app and verified that the data is being replicated via MQ and Event Streams","title":"Summary"},{"location":"faq/","text":"Frequently Asked Questions & Helpful Tips / Tricks \u00b6 How do I get to the workshop OpenShift console? How do I get to the CP4I Platform Navigator? I don't have the TraderLite application, how can I install it? Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous state. Your workshop OpenShift cluster console \u00b6 You will need to access the OpenShift console to install applications used in the lab to interact with the CP4I components . Your instructor will provide you with a link . Click on the link for the OpenShift console. Select the ldapidp user repository Enter the same username and password you used in the previous step and click on Log in . Click on Skip tour when prompted Switch to the Administrator view Click on the link for the project with the same name as your username. The OpenShift console should now look like the following image. Keep this browser tab open as you will need to use the OpenShift console during the labs CP4I Platform Navigator \u00b6 The Cloud Pak for Integration Platform Navigator is a web based application that allows you to access all the components in CP4I. Your instructor will provider you with a URL for the CP4I Platform Navigator and a username and password . You will need this link to access all this CP4I components that you will use in the labs. We recommend that you keep it open in a separate tab. To log in to the Platform Navigator do the following: Click on the link provided to you by your instructor for the Platforn Navigator: Select the Enterprise LDAP user repository, enter your username and password and click on Log in Note: You may have some browser warnings about self-signed certificates. If this happens, continue to the Platform Navigator page anyway. The will launch the CP4I Platform Navigator with links to all the various CP4I components. Keep this tab open for the rest of the workshop as it has all the information and links needed for you to complete the lab exercises. Traderlite Application Installation \u00b6 The traderlite application used in this workshop is installed as part of the API Connect and/or the Salesforce integration labs. If you have not completed that lab, follow these steps to install an instance of the application. In a separate browser tab, go to the OpenShift console of your workshop cluster. Click on Projects in the left navigation and then click on your student001 project in the list. Click on Installed Operators in the left navigation and then click on the TraderLite Operator in the list. Click the Create Instance to start the installation of the TraderLite app. Name the instance traderlite and leave everything else with their default values. Click Create In the left navigation select Pods in the Workloads section and then wait for all the TraderLite pods to have a status of Running and be in the Ready state. Note: You can enter traderlite in the search by name input field to filter the pods.","title":"Frequently asked questions"},{"location":"faq/#frequently-asked-questions-helpful-tips-tricks","text":"How do I get to the workshop OpenShift console? How do I get to the CP4I Platform Navigator? I don't have the TraderLite application, how can I install it? Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous state.","title":"Frequently Asked Questions &amp; Helpful Tips / Tricks"},{"location":"faq/#your-workshop-openshift-cluster-console","text":"You will need to access the OpenShift console to install applications used in the lab to interact with the CP4I components . Your instructor will provide you with a link . Click on the link for the OpenShift console. Select the ldapidp user repository Enter the same username and password you used in the previous step and click on Log in . Click on Skip tour when prompted Switch to the Administrator view Click on the link for the project with the same name as your username. The OpenShift console should now look like the following image. Keep this browser tab open as you will need to use the OpenShift console during the labs","title":"Your workshop OpenShift cluster console"},{"location":"faq/#cp4i-platform-navigator","text":"The Cloud Pak for Integration Platform Navigator is a web based application that allows you to access all the components in CP4I. Your instructor will provider you with a URL for the CP4I Platform Navigator and a username and password . You will need this link to access all this CP4I components that you will use in the labs. We recommend that you keep it open in a separate tab. To log in to the Platform Navigator do the following: Click on the link provided to you by your instructor for the Platforn Navigator: Select the Enterprise LDAP user repository, enter your username and password and click on Log in Note: You may have some browser warnings about self-signed certificates. If this happens, continue to the Platform Navigator page anyway. The will launch the CP4I Platform Navigator with links to all the various CP4I components. Keep this tab open for the rest of the workshop as it has all the information and links needed for you to complete the lab exercises.","title":"CP4I Platform Navigator"},{"location":"faq/#traderlite-application-installation","text":"The traderlite application used in this workshop is installed as part of the API Connect and/or the Salesforce integration labs. If you have not completed that lab, follow these steps to install an instance of the application. In a separate browser tab, go to the OpenShift console of your workshop cluster. Click on Projects in the left navigation and then click on your student001 project in the list. Click on Installed Operators in the left navigation and then click on the TraderLite Operator in the list. Click the Create Instance to start the installation of the TraderLite app. Name the instance traderlite and leave everything else with their default values. Click Create In the left navigation select Pods in the Workloads section and then wait for all the TraderLite pods to have a status of Running and be in the Ready state. Note: You can enter traderlite in the search by name input field to filter the pods.","title":"Traderlite Application Installation"},{"location":"pre-work/","text":"Pre-work \u00b6 Cloud Pak for Integration Platform Navigator \u00b6 The Cloud Pak for Integration Platform Navigator is a web based application that allows you to access all the components in CP4I. Your instructor will provider you with a URL for the CP4I Platform Navigator and a username and password . You will need this link to access all this CP4I components that you will use in the labs. We recommend that you keep it open in a separate tab. To log in to the Platform Navigator do the following: Click on the link provided to you by your instructor for the Platforn Navigator: Select the Enterprise LDAP user repository, enter your username and password and click on Log in Note: You may have some browser warnings about self-signed certificates. If this happens, continue to the Platform Navigator page anyway. The will launch the CP4I Platform Navigator with links to all the various CP4I components. Keep this tab open for the rest of the workshop as it has all the information and links needed for you to complete the lab exercises. OpenShift Cluster Console \u00b6 You will need to access the OpenShift console to install applications used in the lab to interact with the CP4I components . Your instructor will provide you with a link . Click on the link for the OpenShift console. Select the ldapidp user repository Enter the same username and password you used in the previous step and click on Log in . Click on Skip tour when prompted Switch to the Administrator view Click on the link for the project with the same name as your username. The OpenShift console should now look like the following image. Keep this browser tab open as you will need to use the OpenShift console during the labs Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous state.","title":"Environment Setup"},{"location":"pre-work/#pre-work","text":"","title":"Pre-work"},{"location":"pre-work/#cloud-pak-for-integration-platform-navigator","text":"The Cloud Pak for Integration Platform Navigator is a web based application that allows you to access all the components in CP4I. Your instructor will provider you with a URL for the CP4I Platform Navigator and a username and password . You will need this link to access all this CP4I components that you will use in the labs. We recommend that you keep it open in a separate tab. To log in to the Platform Navigator do the following: Click on the link provided to you by your instructor for the Platforn Navigator: Select the Enterprise LDAP user repository, enter your username and password and click on Log in Note: You may have some browser warnings about self-signed certificates. If this happens, continue to the Platform Navigator page anyway. The will launch the CP4I Platform Navigator with links to all the various CP4I components. Keep this tab open for the rest of the workshop as it has all the information and links needed for you to complete the lab exercises.","title":"Cloud Pak for Integration Platform Navigator"},{"location":"pre-work/#openshift-cluster-console","text":"You will need to access the OpenShift console to install applications used in the lab to interact with the CP4I components . Your instructor will provide you with a link . Click on the link for the OpenShift console. Select the ldapidp user repository Enter the same username and password you used in the previous step and click on Log in . Click on Skip tour when prompted Switch to the Administrator view Click on the link for the project with the same name as your username. The OpenShift console should now look like the following image. Keep this browser tab open as you will need to use the OpenShift console during the labs Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous state.","title":"OpenShift Cluster Console"}]}